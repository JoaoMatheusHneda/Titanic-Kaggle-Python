---
title: An R Markdown document converted from "Codigos_Python.ipynb"
output: html_document
---

# Carregando Pacotes

```{python}
import os
import numpy as np
import pandas as pd
from scipy import stats
import sklearn as sl #sl.__version__
from plotnine import *

import scorecardpy as sc
```

```{python}
os.listdir()
```

# Definindo funções

## Funções - Metadados

```{python}
def tabela_metadados(tabela_de_entrada,sufixo_da_tabela_saída,lista_vars_drop):
    
    print('___________________________________________________________________')
    print("O shape da tabela de entrada é: ") 
    print(tabela_de_entrada.shape)

    # Nome das colunas
    metadata_table = pd.DataFrame(tabela_de_entrada.columns)
    metadata_table = metadata_table.rename(columns={0:'columns'})
    
    # Tipo do dado de cada coluna
    metadata_table['dtypes'] = tabela_de_entrada.dtypes.values

    # Número de valores únicos (Cardinalidade)
    metadata_table['nunique'] = tabela_de_entrada.nunique().values
    
    # Número de valores nulos e Porcentagem de valores nulos
    metadata_table['null_value_count'] = tabela_de_entrada.isnull().sum().values
    metadata_table['p_null_value_count'] = (tabela_de_entrada.isnull().sum()/tabela_de_entrada.shape[0]).values
    
    # Porcentagem de valores repetidos da moda em relação aos valores não nulos
    metadata_table['p_max_duplicated_value'] = np.NaN
    for j in range(0,tabela_de_entrada.shape[1]):
        lista = list(tabela_de_entrada.iloc[:,j]) # Transforma a coluna em uma lista
        moda = max(lista,key=lista.count) # Encontra a moda
        maximo_percentage = tabela_de_entrada[tabela_de_entrada.iloc[:,j]==moda].shape[0]/tabela_de_entrada.count()[j]
        metadata_table.loc[j,'p_max_duplicated_value'] = maximo_percentage
    
     # Número de valores únicos (Cardinalidade) (+ 1 se tiver valor nulo)
    metadata_table['nunique_with_null'] = tabela_de_entrada.nunique().values
    nunique_with_null_aux = metadata_table.loc[metadata_table['null_value_count'] > 0].nunique_with_null + 1
    metadata_table.loc[nunique_with_null_aux.index,'nunique_with_null'] = nunique_with_null_aux.values
    
    # Adiciona descrições de cada variável (Para facilitar na hora das legendas, labels e levels dos gráficos - útil na hora de apresentar pra cliente)
    # <em progresso>

    # Reorganiza a ordem das colunas
    metadata_table = metadata_table[['columns','dtypes','nunique','nunique_with_null','null_value_count','p_null_value_count','p_max_duplicated_value']]
    
    metadata_table['drop'] = metadata_table['columns'].isin(lista_vars_drop)

 

    # Salva a tabela na memória
    globals()['metadata_table_' + sufixo_da_tabela_saída] = metadata_table
    print('Tabela ' + 'metadata_table_' + sufixo_da_tabela_saída + ' criada!')
    return globals()['metadata_table_' + sufixo_da_tabela_saída]

```

## Funções - Análise Univariada

### Variável Qualitativa

```{python}
##### Distribuição de Frequências (Variável Qualitativa)

def calcula_tab_frequencia(tabela_de_entrada,nome_variavel):
    
    globals()['table_count_prop' + nome_variavel] = pd.DataFrame(tabela_de_entrada[nome_variavel].value_counts(sort=False))
    globals()['table_count_prop' + nome_variavel][nome_variavel+'_P'] = round((globals()['table_count_prop' + nome_variavel][nome_variavel]/globals()['table_count_prop' + nome_variavel][nome_variavel].sum())*100,2)
    globals()['table_count_prop' + nome_variavel].reset_index(inplace=True)
    globals()['table_count_prop' + nome_variavel] = globals()['table_count_prop' + nome_variavel].sort_values(by='index',ascending=False)
    globals()['table_count_prop' + nome_variavel]['Acc'+nome_variavel] = globals()['table_count_prop' + nome_variavel][nome_variavel].cumsum()
    globals()['table_count_prop' + nome_variavel]['Acc'+nome_variavel+'_P'] = globals()['table_count_prop' + nome_variavel][nome_variavel+'_P'].cumsum()
    print('Tabela ' + 'table_count_prop' + nome_variavel + ' criada!')
    return globals()['table_count_prop' + nome_variavel]

def grafico_tab_frequencia(table_count_prop,nome_variavel,cor_grafico,nudge_x_e,nudge_x_d,size):

    globals()['gg_count_prop' + nome_variavel] = ggplot(aes(x='index',y=nome_variavel),data=table_count_prop) + theme_classic() + \
                                                        geom_col(fill=cor_grafico) + \
                                                        geom_text(aes(label=nome_variavel),                                   
                                                                va='bottom',nudge_x = nudge_x_e, 
                                                                size=size, format_string='{} ') + \
                                                        geom_text(aes(label=nome_variavel+'_P'),                                 
                                                                va='bottom',nudge_x = nudge_x_d,
                                                                size=size, format_string='({}%)')
    print('Gráfico ' + 'gg_count_prop' + nome_variavel + ' criado!')
    return globals()['gg_count_prop' + nome_variavel]


def grafico_tab_frequencia2(table_count_prop,nome_variavel,cor_grafico,size,angle):
    globals()['gg_count_prop' + nome_variavel] = ggplot(aes(x='index',y=nome_variavel),data=table_count_prop) + theme_classic() + \
                                                        geom_col(fill=cor_grafico) + theme(axis_text_x=element_text(angle=angle)) + \
                                                        geom_text(aes(label=nome_variavel+'_P'),                                 
                                                                va='bottom',size=size, format_string='{}%') 
    print('Gráfico ' + 'gg_count_prop' + nome_variavel + ' criado!')
    return globals()['gg_count_prop' + nome_variavel]

def grafico_tab_frequencia3(table_count_prop,nome_variavel,cor_grafico,size1=12,size2=12,angle=45):
    globals()['gg_count_prop' + nome_variavel] = ggplot(aes(x='index',y=nome_variavel),data=table_count_prop) + theme_classic() + \
                                                        geom_col(fill=cor_grafico) + theme(axis_text_x=element_text(angle=angle)) + \
                                                        geom_text(aes(label=nome_variavel),                                 
                                                                va='bottom',size=size1, format_string='{}') + \
                                                        geom_label(aes(label=nome_variavel+'_P'),                               
                                                                nudge_y=-2,va='top',size=size2, format_string='{}%',
                                                                alpha=0.8)# + geom_label(aes(label = nome_variavel+'_P'),va='top')                               
    print('Gráfico ' + 'gg_count_prop' + nome_variavel + ' criado!')
    return globals()['gg_count_prop' + nome_variavel]
```

### Variável Quantitativa

```{python}
##### Distribuição de Frequências (Variável Quantitativa)

# Cálculo do número de categorias (Sturges)
# def calcula_num_categorias_sturges(n):
#     global k
#     k = int(round(1 + 3.322 * np.log10(n), 0))
#     return (n,k)

# Definindo os intervalos de um histograma
def binning_quantities(dados_entrada,nome_variavel,nome_variavel_resposta,method='Sturges'):
    #global dados_saida
    # dados_entrada # Dados para calculo dos intervalos
    #dados_entrada = df[df.Sample == 'train']['variavel']

    if method == 'Sturges':
        n = dados_entrada[~dados_entrada[nome_variavel].isnull()].shape[0]
        k = int(round(1 + 3.322 * np.log10(n), 0))
        interval = pd.cut(dados_entrada[nome_variavel], k, include_lowest=True,retbins=True)
        print('Método Sturges : n (desconsidera nulos) = ',n,'|> k (desconsidera nulos) = ',k)
    elif method == 'Percentiles':
        #percentiles = np.array(list(range(0,105,5)))/100 # 20 faixas
        percentiles = np.array(list(range(0,110,10)))/100 # 10 faixas
        interval = pd.qcut(dados_entrada[nome_variavel], percentiles, retbins=True)
        print('Método Percentiles:')

    # Inclui os valores np.-inf e np.inf nos intervalos
    interval = interval[1] # limites
    interval = interval[1:interval.shape[0]-1]
    globals()['limites_intervalos' + nome_variavel] = list([-np.Inf] + list(interval) + [np.Inf])
    globals()['limites_intervalos' + nome_variavel] = [round(i,2) for i in globals()['limites_intervalos' + nome_variavel]]
    bins = pd.IntervalIndex.from_breaks(globals()['limites_intervalos' + nome_variavel])
    #bins

    #-------------------------------------------------------------------------

    # dados_aplicacao_saida # Dados que serão utilizados para a aplicação dos intervalos calculados
    #dados_aplicacao_saida = df['variavel']

    # dados_saida # Dados alterados com a aplicação dos intervalos calculados
    dados_saida =  pd.cut(dados_entrada[nome_variavel], bins) # Faz a aplicação de acordo com os intervalos calculados

    # # Adicionando a categoria Missing aos intervalos
    dados_saida = dados_saida.cat.add_categories(['Missing'])

    # # Reorganizando os intervalos
    categorias = list(dados_saida.cat.categories.values)
    categorias_missing = [dados_saida.cat.categories.values[-1]]
    categorias_missing.extend(list(dados_saida.cat.categories.values[:-1]))

    dados_saida = dados_saida.cat.reorder_categories(categorias_missing)

    # # # Substitui os valores nulos por 'Missing'
    dados_saida = dados_saida.fillna('Missing')
    print(dados_saida)

    print('--------------------------------------------------------------------------------------------------')
    dados_saida = pd.DataFrame(dados_saida)
    dados_saida = pd.DataFrame(dados_saida[nome_variavel].values,dados_entrada[nome_variavel_resposta].values).reset_index().rename(columns={'index':nome_variavel_resposta,0:nome_variavel})
    print(dados_saida.dtypes)
    
    print('-------------------------------------------------')
    print('Tabela dados_saida' + nome_variavel + ' e lista limites_intervalos' + nome_variavel + ' criadas!')

    globals()['dados_saida' + nome_variavel] = dados_saida
    return globals()['dados_saida' + nome_variavel], globals()['limites_intervalos' + nome_variavel]

def apply_binning(dados_entrada,nome_variavel,breaks):
    bins = pd.IntervalIndex.from_breaks(breaks)
    #-------------------------------------------------------------------------

    # dados_aplicacao_saida # Dados que serão utilizados para a aplicação dos intervalos calculados
    #dados_aplicacao_saida = df['variavel']

    # dados_saida # Dados alterados com a aplicação dos intervalos calculados
    dados_saida =  pd.cut(dados_entrada[nome_variavel], bins) # Faz a aplicação de acordo com os intervalos calculados

    # # Adicionando a categoria Missing aos intervalos
    dados_saida = dados_saida.cat.add_categories(['Missing'])

    # # Reorganizando os intervalos
    categorias = list(dados_saida.cat.categories.values)
    categorias_missing = [dados_saida.cat.categories.values[-1]]
    categorias_missing.extend(list(dados_saida.cat.categories.values[:-1]))

    dados_saida = dados_saida.cat.reorder_categories(categorias_missing)

    # # # Substitui os valores nulos por 'Missing'
    dados_saida = dados_saida.fillna('Missing')
    print(dados_saida)

    print('--------------------------------------------------------------------------------------------------')
    dados_saida = pd.DataFrame(dados_saida)
    print(dados_saida.dtypes)
    
    print('-------------------------------------------------')
    print('Tabela dados_saida' + nome_variavel + ' criada!')
    
    print('-------------------------------------------------')
    print('Coluna',nome_variavel,'da tabela de entrada original modificada!')
    
    globals()['dados_saida' + nome_variavel] = dados_saida
    dados_entrada[[nome_variavel]] = dados_saida
    return globals()['dados_saida' + nome_variavel],dados_entrada[nome_variavel]
```

## Funções - Análise Bivariada

```{python}
def calcula_tab_frequencia_bivariada(tabela_de_entrada,nome_variavel1,nome_variavel2):
   
    # tabela_de_entrada # df[df.Sample == 'train']
    # nome_variavel1 # 'Survived'
    # nome_variavel2 # 'Pclass'

    # Tabela de Frequência Bivariada
    globals()['table_count_prop' + nome_variavel1 + '_' + nome_variavel2] = pd.DataFrame(tabela_de_entrada[[nome_variavel1, nome_variavel2]].value_counts(sort=False)).reset_index()
    globals()['table_count_prop' + nome_variavel1 + '_' + nome_variavel2] = globals()['table_count_prop' + nome_variavel1 + '_' + nome_variavel2].rename(columns={0:'Contagem'})

    # Acrescenta a soma total de cada nível da covariável
    globals()['table_count_prop' + nome_variavel1 + '_' + nome_variavel2] = globals()['table_count_prop' + nome_variavel1 + '_' + nome_variavel2].merge(globals()['table_count_prop' + nome_variavel1 + '_' + nome_variavel2].groupby(nome_variavel2)['Contagem'].sum().reset_index().rename(columns={'Contagem':'Sum' + nome_variavel2}),on=nome_variavel2)
    
    # Acrescenta a soma acumulada total de cada nível da covariável
    cumsum_aux = globals()['table_count_prop' + nome_variavel1 + '_' + nome_variavel2].groupby(nome_variavel2)['Contagem'].sum().reset_index()
    cumsum_aux['AccSum' + nome_variavel2] = cumsum_aux['Contagem'].cumsum()
    cumsum_aux['AccSum' + nome_variavel2 + '_P'] = round((cumsum_aux['Contagem']/cumsum_aux['Contagem'].sum())*100,2)
    globals()['table_count_prop' + nome_variavel1 + '_' + nome_variavel2] = globals()['table_count_prop' + nome_variavel1 + '_' + nome_variavel2].merge(cumsum_aux[[nome_variavel2,'AccSum' + nome_variavel2,'AccSum' + nome_variavel2 + '_P']],on=nome_variavel2)

    # Acrescenta a proporção da resposta em relação a cada nível da covariável 
    globals()['table_count_prop' + nome_variavel1 + '_' + nome_variavel2][nome_variavel1 + '/' + 'Sum' + nome_variavel2] = round((globals()['table_count_prop' + nome_variavel1 + '_' + nome_variavel2]['Contagem']/globals()['table_count_prop' + nome_variavel1 + '_' + nome_variavel2]['Sum' + nome_variavel2]),4)
    globals()['table_count_prop' + nome_variavel1 + '_' + nome_variavel2][nome_variavel1 + '/' + 'Sum' + nome_variavel2 + '_P'] = round(globals()['table_count_prop' + nome_variavel1 + '_' + nome_variavel2][nome_variavel1 + '/' + 'Sum' + nome_variavel2]*100,1)
    
    print('Tabela table_count_prop' + nome_variavel1 + '_' + nome_variavel2 + ' criada!')
    return globals()['table_count_prop' + nome_variavel1 + '_' + nome_variavel2]

def grafico_tab_frequencia_bivariada1(table_count_prop,nome_variavel1,nome_variavel2,angle=0):
    globals()['gg_count_prop' + nome_variavel1 + '_' + nome_variavel2 + '1'] = ggplot(aes(x=nome_variavel2,y='Contagem',fill=nome_variavel1),data=table_count_prop) + theme_classic() + \
                                                                                geom_col() + theme(axis_text_x=element_text(angle=angle)) + \
                                                                                geom_text(aes(y='Sum' + nome_variavel2,label='Sum' + nome_variavel2),
                                                                                    data=table_count_prop,
                                                                                    nudge_x=-0.2,va = 'bottom', format_string='{} /') + \
                                                                                geom_text(aes(y='Sum' + nome_variavel2,label='AccSum' + nome_variavel2 + '_P'),
                                                                                    data=table_count_prop,
                                                                                    nudge_x=0.2,va='bottom',format_string='({}%)')
    print('Gráfico ' + 'gg_count_prop' + nome_variavel1 + '_' + nome_variavel2 + '1' + ' criado!')
    return globals()['gg_count_prop' + nome_variavel1 + '_' + nome_variavel2 + '1']

def grafico_tab_frequencia_bivariada2(table_count_prop,nome_variavel1,nome_variavel2,size1=12,size2=12,angle=0):
    if size2 != 0:
        print('Para retirar as porcentagens, use size2 = 0!')
    elif size2 == 0:
        print('Para adicionar as porcentagens, use size2 > 0!')
    
    globals()['gg_count_prop' + nome_variavel1 + '_' + nome_variavel2 + '2'] = ggplot(aes(x=nome_variavel2,y='Contagem',fill=nome_variavel1),data=table_count_prop) + theme_classic() + ylim(0,round(table_count_prop['Contagem'].max()*1.1,0)) + \
                                                                                geom_col(position="dodge") + theme(axis_text_x=element_text(angle=angle)) + \
                                                                                geom_text(aes(label='Contagem'),data=table_count_prop,
                                                                                size=size1, format_string='{}',
                                                                                position = position_dodge(0.9),va='bottom') + \
                                                                                geom_text(aes(y='Contagem',label=nome_variavel1 + '/' + 'Sum' + nome_variavel2 + '_P'),data=table_count_prop,
                                                                                size=size2, format_string='({}%) \n',
                                                                                position = position_dodge(0.9),va='bottom',color='black')
    print('Gráfico ' + 'gg_count_prop' + nome_variavel1 + '_' + nome_variavel2 + '2' + ' criado!')
    return globals()['gg_count_prop' + nome_variavel1 + '_' + nome_variavel2 + '2']

def grafico_tab_frequencia_bivariada3(table_count_prop,nome_variavel1,nome_variavel2,size1=12,size2=12,angle=0):
    if size2 != 0:
        print('Para retirar as contagens, use size2 = 0!')
    elif size2 == 0:
        print('Para adicionar as contagens, use size2 > 0!')
    
    globals()['gg_count_prop' + nome_variavel1 + '_' + nome_variavel2 + '3'] = ggplot(aes(x=nome_variavel2,y='Contagem',fill=nome_variavel1),data=table_count_prop) + theme_classic() + \
                                                                                geom_col(position="fill",stat="identity") + theme(axis_text_x=element_text(angle=angle)) + \
                                                                                geom_text(aes(y=nome_variavel1 + '/' + 'Sum' + nome_variavel2,label=nome_variavel1 + '/' + 'Sum' + nome_variavel2 + '_P'),data=table_count_prop,
                                                                                size=size1, format_string='{}%',position = "stack",va='bottom') + \
                                                                                geom_text(aes(y=nome_variavel1 + '/' + 'Sum' + nome_variavel2,label='Contagem'),data=table_count_prop,
                                                                                size=size2, format_string='{}',position = "stack",va='top',color='black')
    print('Gráfico ' + 'gg_count_prop' + nome_variavel1 + '_' + nome_variavel2 + '3' + ' criado!')
    return globals()['gg_count_prop' + nome_variavel1 + '_' + nome_variavel2 + '3']
```

# Importando os dados e juntando as tabelas

```{python}
train = pd.read_csv('../train.csv',sep = ",")
test = pd.read_csv('../test.csv',sep = ",")
print(train.shape)
print(test.shape)
print(train.shape[0] + test.shape[0])
```

```{python}
train['Sample'] = 'train'

test['Survived'] = np.nan
test['Sample'] = 'test'
test = test[train.columns]
```

```{python}
df = pd.concat([train,test])
```

# Pré-processamento para a tabela de Metadados

```{python}
df.dtypes
```

```{python}
df
```

Alteração do dtypes

```{python}
df['Survived'] = df['Survived'].astype('category')
df['Pclass'] = pd.Series(pd.Categorical(df['Pclass'],categories=[1,2,3], ordered=True))
df['Sex'] = df['Sex'].astype('category')
df['Cabin'] = df['Cabin'].astype('category')
df['Embarked'] = df['Embarked'].astype('category')
```

# Tabela de Metadados (Foco na Análise Exploratória e Tratamento dos dados)

```{python}
tabela_metadados(tabela_de_entrada=df[df.Sample == 'train'],
                 sufixo_da_tabela_saída='df_train',
                 lista_vars_drop=['PassengerId','Name','Ticket'])
```

```{python}
tabela_metadados(tabela_de_entrada=df[df.Sample == 'test'],
                 sufixo_da_tabela_saída='df_test',
                 lista_vars_drop=['PassengerId','Name','Ticket'])
```

```{python}
tabela_metadados(tabela_de_entrada=df,
                 sufixo_da_tabela_saída='df',
                 lista_vars_drop=['PassengerId','Name','Ticket'])
```

# Análise Descritiva e Exploratória

## Análise de Correlação

```{python}
train.corr(method='pearson')
```

```{python}
train.corr(method='spearman')
```

```{python}
train.corr(method='spearman')
```

```{python}
test.corr(method='pearson')
```

```{python}
test.corr(method='spearman')
```

## Análise Univariada

### Sample

```{python}
calcula_tab_frequencia(tabela_de_entrada=df,
                       nome_variavel='Sample')
```

Reordenando as categorias para plotar no gráfico

```{python}
table_count_propSample_new_sort = table_count_propSample.copy()
#table_count_propCabin['index'][[1,5] + list(table_count_propCabin['index'].index[2:].values)]
table_count_propSample_new_sort['index'] = pd.Categorical(table_count_propSample_new_sort['index'])
table_count_propSample_new_sort['index'] = table_count_propSample_new_sort['index'].cat.reorder_categories(['train','test'])
table_count_propSample_new_sort['index']
```

```{python}
grafico_tab_frequencia(table_count_prop=table_count_propSample_new_sort,
                       nome_variavel='Sample',
                       cor_grafico='black',
                       nudge_x_e=-0.10,
                       nudge_x_d=0.14,
                       size=12)
```

### Survived (Resposta)

```{python}
calcula_tab_frequencia(tabela_de_entrada=df[df.Sample == 'train'],
                       nome_variavel='Survived')
```

```{python}
grafico_tab_frequencia(table_count_prop=table_count_propSurvived,
                       nome_variavel='Survived',
                       cor_grafico='blue',
                       nudge_x_e=-0.10,
                       nudge_x_d=0.14,
                       size=12)
```

## Análise Univariada e Bivariada

### Pclass

```{python}
calcula_tab_frequencia(tabela_de_entrada=df[df.Sample == 'train'],
                       nome_variavel='Pclass')
```

```{python}
grafico_tab_frequencia(table_count_prop=table_count_propPclass,
                       nome_variavel='Pclass',
                       cor_grafico='red',
                       nudge_x_e=-0.2,
                       nudge_x_d=0.14,
                       size=12)
```

### Survived x Pclass

```{python}
calcula_tab_frequencia_bivariada(df[df.Sample == 'train'],'Survived','Pclass')
```

```{python}
grafico_tab_frequencia_bivariada2(table_count_propSurvived_Pclass,'Survived','Pclass',angle=0,size1=12,size2=12)
```

```{python}
grafico_tab_frequencia_bivariada3(table_count_propSurvived_Pclass,'Survived','Pclass',size1=12,size2=0,angle=0)
```

## Sex

```{python}
calcula_tab_frequencia(tabela_de_entrada=df[df.Sample == 'train'],
                       nome_variavel='Sex')
```

```{python}
grafico_tab_frequencia(table_count_prop=table_count_propSex,
                       nome_variavel='Sex',
                       cor_grafico='green',
                       nudge_x_e=-0.1,
                       nudge_x_d=0.14,
                       size=12)
```

### Survived x Sex

```{python}
calcula_tab_frequencia_bivariada(df[df.Sample == 'train'],'Survived','Sex')
```

```{python}
grafico_tab_frequencia_bivariada2(table_count_propSurvived_Sex,'Survived','Sex',angle=0,size1=12,size2=12)
```

```{python}
grafico_tab_frequencia_bivariada3(table_count_propSurvived_Sex,'Survived','Sex',size1=12,size2=0,angle=0)
```

## Age

### Binning para visualização **(Método Sturges)**

### Cálculo do número de categorias (**Sturges**)

```{python}
binning_quantities(dados_entrada=df[df.Sample == 'train'],
                   nome_variavel='Age',
                   nome_variavel_resposta='Survived',
                   method='Sturges')
```

```{python}
calcula_tab_frequencia(tabela_de_entrada=dados_saidaAge,
                       nome_variavel='Age')
```

```{python}
grafico_tab_frequencia3(table_count_prop=table_count_propAge,
                       nome_variavel='Age',
                       cor_grafico='green',
                       size1=12,
                       size2=8,
                       angle=45)
```

### Análise Bivariada

```{python}
df[df.Sample == 'train'].groupby(['Survived'])['Age'].describe()
```

```{python}
ggplot(aes(x='Survived',y='Age'),data=df[df.Sample == 'train']) + geom_boxplot() + coord_flip()
```

```{python}
calcula_tab_frequencia_bivariada(dados_saidaAge,'Survived','Age')
```

```{python}
grafico_tab_frequencia_bivariada2(table_count_propSurvived_Age,'Survived','Age',size1=9,size2=0,angle=45)
```

```{python}
grafico_tab_frequencia_bivariada3(table_count_propSurvived_Age,'Survived','Age',size1=9,size2=0,angle=45)
```

#### Formata a variável resposta

```{python}
df_for_woe = df.copy()
df_for_woe['Survived_BadGod'] = df_for_woe['Survived'].replace({0: 'bad', 1: 'good'})
```

### Binning 'Automático' **(Método Optimal Binning)**

```{python}
# woe binning ------
bins = sc.woebin(df_for_woe[df_for_woe.Sample == 'train'],
                x='Age',
                y='Survived_BadGod',
                method='tree',# "tree" and "chimerge" for optimal binning (numerical e categorical), # width' and 'freq' for equal binning that support numerical variables only.
                stop_limit=0.05)
bins['Age']['woe_period1'] = bins['Age']['woe'].shift(periods=1)
bins['Age']['woe_diff'] = bins['Age']['woe_period1'] - bins['Age']['woe']
bins['Age']['P_j/good'] = bins['Age']['good']/bins['Age']['good'].sum()
bins['Age']['P_j/bad'] = bins['Age']['bad']/bins['Age']['bad'].sum()
bins['Age']['RR'] = bins['Age']['P_j/good']/bins['Age']['P_j/bad']
bins['Age']['f*'] = np.log(bins['Age']['RR'])
bins['Age']['V(f*)'] = ((1-bins['Age']['P_j/good'])/(bins['Age']['good'].sum()*bins['Age']['P_j/good'])) + ((1-bins['Age']['P_j/bad'])/(bins['Age']['bad'].sum()*bins['Age']['P_j/bad']))
bins['Age']['RR_IC_low'] = np.exp(bins['Age']['f*'] - stats.norm.ppf(.975, loc=0, scale=1)*np.sqrt(bins['Age']['V(f*)']))
bins['Age']['RR_IC_Hight'] = np.exp(bins['Age']['f*'] + stats.norm.ppf(.975, loc=0, scale=1)*np.sqrt(bins['Age']['V(f*)']))
bins['Age']
```

```{python}
ggplot(aes(x='RR',y='bin'),data=bins['Age']) + geom_point(colour='red') + \
    geom_segment(aes(x = 'RR_IC_low', y = 'bin', xend = 'RR_IC_Hight', yend = 'bin'),colour='red')
```

```{python}
sc.woebin_plot(bins)
```

### Binning 'Manual' **(Método Percentiles) - Antes da fusão de classes**

```{python}
binning_quantities(dados_entrada=df[df.Sample == 'train'],
                   nome_variavel='Age',
                   nome_variavel_resposta='Survived',
                   method='Percentiles')
```

```{python}
calcula_tab_frequencia(tabela_de_entrada=dados_saidaAge,
                       nome_variavel='Age')
```

```{python}
breaks_list = {
    'Age': limites_intervalosAge,
}
# woe binning ------
bins = sc.woebin(df_for_woe[df_for_woe.Sample == 'train'],
                breaks_list=breaks_list,
                #special_values=,
                x='Age',
                y='Survived_BadGod')
                
bins['Age']['woe_period1'] = bins['Age']['woe'].shift(periods=1)
bins['Age']['woe_diff'] = bins['Age']['woe_period1'] - bins['Age']['woe']
bins['Age']['P_j/good'] = bins['Age']['good']/bins['Age']['good'].sum()
bins['Age']['P_j/bad'] = bins['Age']['bad']/bins['Age']['bad'].sum()
bins['Age']['RR'] = bins['Age']['P_j/good']/bins['Age']['P_j/bad']
bins['Age']['f*'] = np.log(bins['Age']['RR'])
bins['Age']['V(f*)'] = ((1-bins['Age']['P_j/good'])/(bins['Age']['good'].sum()*bins['Age']['P_j/good'])) + ((1-bins['Age']['P_j/bad'])/(bins['Age']['bad'].sum()*bins['Age']['P_j/bad']))
bins['Age']['RR_IC_low'] = np.exp(bins['Age']['f*'] - stats.norm.ppf(.975, loc=0, scale=1)*np.sqrt(bins['Age']['V(f*)']))
bins['Age']['RR_IC_Hight'] = np.exp(bins['Age']['f*'] + stats.norm.ppf(.975, loc=0, scale=1)*np.sqrt(bins['Age']['V(f*)']))

bins['Age']
```

```{python}
sc.woebin_plot(bins)
```

```{python}
ggplot(aes(x='RR',y='bin'),data=bins['Age']) + geom_point(color='red') + \
    geom_segment(aes(x = 'RR_IC_low', y = 'bin', xend = 'RR_IC_Hight', yend = 'bin'),color='red')
```

### Binning 'Manual' **(Método Percentiles) - Depois da fusão de classes**

```{python}
breaks_list = {
    'Age': [-np.inf,19.0,np.inf],
}
# woe binning ------
bins = sc.woebin(df_for_woe[df_for_woe.Sample == 'train'],
                breaks_list=breaks_list,
                #special_values=,
                x='Age',
                y='Survived_BadGod')
bins['Age']['woe_period1'] = bins['Age']['woe'].shift(periods=1)
bins['Age']['woe_diff'] = bins['Age']['woe_period1'] - bins['Age']['woe']
bins['Age']['P_j/good'] = bins['Age']['good']/bins['Age']['good'].sum()
bins['Age']['P_j/bad'] = bins['Age']['bad']/bins['Age']['bad'].sum()
bins['Age']['RR'] = bins['Age']['P_j/good']/bins['Age']['P_j/bad']
bins['Age']['f*'] = np.log(bins['Age']['RR'])
bins['Age']['V(f*)'] = ((1-bins['Age']['P_j/good'])/(bins['Age']['good'].sum()*bins['Age']['P_j/good'])) + ((1-bins['Age']['P_j/bad'])/(bins['Age']['bad'].sum()*bins['Age']['P_j/bad']))
bins['Age']['RR_IC_low'] = np.exp(bins['Age']['f*'] - stats.norm.ppf(.975, loc=0, scale=1)*np.sqrt(bins['Age']['V(f*)']))
bins['Age']['RR_IC_Hight'] = np.exp(bins['Age']['f*'] + stats.norm.ppf(.975, loc=0, scale=1)*np.sqrt(bins['Age']['V(f*)']))
bins['Age']
```

```{python}
sc.woebin_plot(bins)
```

```{python}
ggplot(aes(x='RR',y='bin'),data=bins['Age']) + geom_point(colour='blue') + \
    geom_segment(aes(x = 'RR_IC_low', y = 'bin', xend = 'RR_IC_Hight', yend = 'bin'),colour='blue')
```

**Dicas para discretização via WOE:**
1) Todas as categorias calculadas têm pelo menos 5% das observações.
2) Todas as categorias calculadas têm valores diferentes de zero para o número de casos bons e ruins.
3) Os Woe's são distintos para cada categoria calculada. As categorias calculadas que tinham Woe's semelhantes foram agrupadas.
4) Valores ausentes estão em categorias separadas.
5) Os Woe's são monotônicos, isto é, estão crescendo ou diminuindo com os agrupamentos. (Recomendação do uso dessa dica é mais fraca do que para as outras dicas).

O método **Binning 'Automático' (Método Optimal Binning)** ganha do método **Binning 'Manual' (Método Percentiles) - Antes da fusão de classes** e **Binning 'Manual' (Método Percentiles) - Depois da fusão de classes** no quesito valor da informação (0.146078 > 0.127798 > 0.069084), porém somente o método **Binning 'Manual' (Método Percentiles) - Depois da fusão de classes** obedece o quesito de monotonicidade (uma das dicas na discretização via WOE). Por conta da facilidade de interpretação da relação entre Age e Survived, optou-se por considerar a discretização via **Binning 'Manual' (Método Percentiles) - Depois da fusão de classes**.

### Aplicando as modificações nos dados

```{python}
breaks = bins['Age']['breaks'][1:].values.astype(np.float64)
breaks = np.append(-np.inf,breaks)

apply_binning(df_for_woe,'Age',breaks)
```

```{python}
df_for_woe.head()
```

```{python}
calcula_tab_frequencia(tabela_de_entrada=dados_saidaAge,
                       nome_variavel='Age')
```

```{python}
grafico_tab_frequencia3(table_count_prop=table_count_propAge,
                       nome_variavel='Age',
                       cor_grafico='green',
                       size1=12,
                       size2=8,
                       angle=45)
```

```{python}
calcula_tab_frequencia_bivariada(df_for_woe[df_for_woe.Sample == 'train'],'Survived','Age')
```

```{python}
grafico_tab_frequencia_bivariada2(table_count_propSurvived_Age,'Survived','Age',size1=9,size2=9,angle=45)
```

```{python}
grafico_tab_frequencia_bivariada3(table_count_propSurvived_Age,'Survived','Age',size1=9,size2=0,angle=45)
```

## SibSp

```{python}
calcula_tab_frequencia(tabela_de_entrada=df[df.Sample == 'train'],
                       nome_variavel='SibSp')
```

```{python}
grafico_tab_frequencia2(table_count_prop=table_count_propSibSp,
                       nome_variavel='SibSp',
                       cor_grafico='orange',
                       size=9,
                       angle=0)
```

```{python}
calcula_tab_frequencia_bivariada(df[df.Sample == 'train'],'Survived','SibSp')
```

```{python}
grafico_tab_frequencia_bivariada2(table_count_propSurvived_SibSp,'Survived','SibSp',size1=9,size2=0,angle=45)
```

```{python}
grafico_tab_frequencia_bivariada3(table_count_propSurvived_SibSp,'Survived','SibSp',size1=9,size2=0,angle=45)
```

## Parch

```{python}
calcula_tab_frequencia(tabela_de_entrada=df[df.Sample == 'train'],
                       nome_variavel='Parch')
```

```{python}
grafico_tab_frequencia2(table_count_prop=table_count_propParch,
                       nome_variavel='Parch',
                       cor_grafico='purple',
                       size=9,
                       angle=0)
```

```{python}
calcula_tab_frequencia_bivariada(df[df.Sample == 'train'],'Survived','Parch')
```

```{python}
grafico_tab_frequencia_bivariada2(table_count_propSurvived_Parch,'Survived','Parch',size1=9,size2=0,angle=45)
```

```{python}
grafico_tab_frequencia_bivariada3(table_count_propSurvived_Parch,'Survived','Parch',size1=9,size2=0,angle=45)
```

## Fare

```{python}
df[df.Sample == 'train']['Fare'].describe()
```

```{python}
binning_quantities(dados_entrada=df[df.Sample == 'train'],
                   nome_variavel='Fare',
                   nome_variavel_resposta='Survived',
                   method='Sturges')
```

```{python}
calcula_tab_frequencia(tabela_de_entrada=dados_saidaFare,
                       nome_variavel='Fare')
```

```{python}
grafico_tab_frequencia3(table_count_prop=table_count_propFare,
                       nome_variavel='Fare',
                       cor_grafico='blue',
                       size1=12,
                       size2=8,
                       angle=45)
```

```{python}
df[df.Sample == 'train'].groupby(['Survived'])['Fare'].describe()
```

```{python}
ggplot(aes(x='Survived',y='Fare'),data=df[df.Sample == 'train']) + geom_boxplot() + coord_flip()
```

# Preenchendo o dado faltante (Mr. Thomas Storey)

```{python}
df_for_woe[df_for_woe['Fare'].isnull()]
```

Uma das técnicas mais rápidas e menos trabalhosas para substituir um dado faltante é a utilização das próprias medidas de posição, apesar de existirem outras técnicas com melhor fundamentação teórica. Considerando que para o caso da variável Fare existe somente um dado faltante, essa técnica foi a escolhida. Além disso, visto que os dados de Fare e Pclass estão negativamente correlacionados, verificou-se o comportamento dessas duas variáveis para que um valor mais lógico pudesse ser utilizado para substituir o dado faltante.

```{python}
ggplot(aes(y='Fare',x='Pclass'),data=df_for_woe[df_for_woe.Sample == 'train']) + geom_boxplot()
```

A medida de posição escolhida foi a mediana. Além disso, como o indivíduo pertence a terceira classe, foi escolhida a mediana da terceira classe para substituir o dado faltante.

```{python}
df_for_woe[df_for_woe.Sample == 'train'].groupby(['Pclass'])['Fare'].describe()
```

```{python}
df_for_woe['Fare'] = df_for_woe['Fare'].fillna(8.0500)
```

```{python}
df_for_woe[df_for_woe['PassengerId'] == 1044]
```

## Cabin

#### Formata a covariável Cabin

```{python}
df_for_woe['Cabin'] = df_for_woe['Cabin'].cat.add_categories(['Missing']).fillna('Missing')
```

```{python}
df_for_woe[df_for_woe.Sample == 'train']['Cabin'].cat.categories.values
```

### **Binning 'Manual' - Com fusão de classes**

Agrupando as cabines

Possivelmente existem grupos de cabines com pessoas que têm maiores ou menores chances de sobrevivência. Por conta disso, esse agrupamento foi feito para verificar mais facilmente essa hipótese.

```{python}
df_for_woe['Cabin'] = df_for_woe['Cabin'].str[0].replace({'M':'Missing'})
df_for_woe['Cabin'] = df_for_woe['Cabin'].astype('category')
df_for_woe['Cabin'] = df_for_woe['Cabin'].cat.reorder_categories(['A','B','C','D','E','F','G','T','Missing'])
df_for_woe['Cabin'].cat.categories.values
```

```{python}
calcula_tab_frequencia(tabela_de_entrada=df_for_woe[df_for_woe.Sample == 'train'],
                       nome_variavel='Cabin')
```

```{python}
grafico_tab_frequencia2(table_count_prop=table_count_propCabin,
                       nome_variavel='Cabin',
                       cor_grafico='purple',
                       size=9,
                       angle=0)
```

```{python}
df_for_woe[df_for_woe.Sample == 'train']['Cabin'].cat.categories.values
```

```{python}
breaks_list = {
    'Cabin': df_for_woe[df_for_woe.Sample == 'train']['Cabin'].cat.categories.values#(pd.DataFrame(df[df.Sample == 'train']['Cabin'].astype('object')).fillna('Missing'))['Cabin'].unique(),
}
# woe binning ------
bins = sc.woebin(df_for_woe[df_for_woe.Sample == 'train'],
                breaks_list=breaks_list,
                #special_values=,
                x='Cabin',
                y='Survived_BadGod')
bins['Cabin']['woe_period1'] = bins['Cabin']['woe'].shift(periods=1)
bins['Cabin']['woe_diff'] = bins['Cabin']['woe_period1'] - bins['Cabin']['woe']
bins['Cabin']['P_j/good'] = bins['Cabin']['good']/bins['Cabin']['good'].sum()
bins['Cabin']['P_j/bad'] = bins['Cabin']['bad']/bins['Cabin']['bad'].sum()
bins['Cabin']['RR'] = bins['Cabin']['P_j/good']/bins['Cabin']['P_j/bad']
bins['Cabin']['f*'] = np.log(bins['Cabin']['RR'])
bins['Cabin']['V(f*)'] = ((1-bins['Cabin']['P_j/good'])/(bins['Cabin']['good'].sum()*bins['Cabin']['P_j/good'])) + ((1-bins['Cabin']['P_j/bad'])/(bins['Cabin']['bad'].sum()*bins['Cabin']['P_j/bad']))
bins['Cabin']['RR_IC_low'] = np.exp(bins['Cabin']['f*'] - stats.norm.ppf(.975, loc=0, scale=1)*np.sqrt(bins['Cabin']['V(f*)']))
bins['Cabin']['RR_IC_Hight'] = np.exp(bins['Cabin']['f*'] + stats.norm.ppf(.975, loc=0, scale=1)*np.sqrt(bins['Cabin']['V(f*)']))
bins['Cabin']
```

```{python}
sc.woebin_plot(bins)
```

### **Método Optimal Binning - A partir das classes agregadas**

Essa junção foi feita a partir do **Binning 'Manual' - Com fusão de classes**. Isso foi feito para verificar se faria sentido diminuir o detalhamento das cabines para facilitar a interpretabilidade.

```{python}
# woe binning ------
bins = sc.woebin(df_for_woe[df_for_woe.Sample == 'train'],
                x='Cabin',
                y='Survived_BadGod',
                method='tree',# "tree" and "chimerge" for optimal binning (numerical e categorical), # width' and 'freq' for equal binning that support numerical variables only.
                stop_limit=0.05)

bins['Cabin']['woe_period1'] = bins['Cabin']['woe'].shift(periods=1)
bins['Cabin']['woe_diff'] = bins['Cabin']['woe_period1'] - bins['Cabin']['woe']
bins['Cabin']['P_j/good'] = bins['Cabin']['good']/bins['Cabin']['good'].sum()
bins['Cabin']['P_j/bad'] = bins['Cabin']['bad']/bins['Cabin']['bad'].sum()
bins['Cabin']['RR'] = bins['Cabin']['P_j/good']/bins['Cabin']['P_j/bad']
bins['Cabin']['f*'] = np.log(bins['Cabin']['RR'])
bins['Cabin']['V(f*)'] = ((1-bins['Cabin']['P_j/good'])/(bins['Cabin']['good'].sum()*bins['Cabin']['P_j/good'])) + ((1-bins['Cabin']['P_j/bad'])/(bins['Cabin']['bad'].sum()*bins['Cabin']['P_j/bad']))
bins['Cabin']['RR_IC_low'] = np.exp(bins['Cabin']['f*'] - stats.norm.ppf(.975, loc=0, scale=1)*np.sqrt(bins['Cabin']['V(f*)']))
bins['Cabin']['RR_IC_Hight'] = np.exp(bins['Cabin']['f*'] + stats.norm.ppf(.975, loc=0, scale=1)*np.sqrt(bins['Cabin']['V(f*)']))
bins['Cabin']
```

```{python}
sc.woebin_plot(bins)
```

### **Binning 'Manual' - Ajuste do** *Método Optimal Binning -  A partir das classes agregadas*

Essa última junção foi feita a partir do **Método Optimal Binning - A partir das classes agregadas**. A única alteração que foi feita foi manter a classe missing isolada.

```{python}
breaks_list = {
    'Cabin': ["A%,%B%,%C","D%,%E%,%F%,%G%,%T"]#(pd.DataFrame(df[df.Sample == 'train']['Cabin'].astype('object')).fillna('Missing'))['Cabin'].unique(),
}
# woe binning ------
bins = sc.woebin(df_for_woe[df_for_woe.Sample == 'train'],
                breaks_list=breaks_list,
                #special_values=,
                x='Cabin',
                y='Survived_BadGod')
bins['Cabin']['woe_period1'] = bins['Cabin']['woe'].shift(periods=1)
bins['Cabin']['woe_diff'] = bins['Cabin']['woe_period1'] - bins['Cabin']['woe']
bins['Cabin']['P_j/good'] = bins['Cabin']['good']/bins['Cabin']['good'].sum()
bins['Cabin']['P_j/bad'] = bins['Cabin']['bad']/bins['Cabin']['bad'].sum()
bins['Cabin']['RR'] = bins['Cabin']['P_j/good']/bins['Cabin']['P_j/bad']
bins['Cabin']['f*'] = np.log(bins['Cabin']['RR'])
bins['Cabin']['V(f*)'] = ((1-bins['Cabin']['P_j/good'])/(bins['Cabin']['good'].sum()*bins['Cabin']['P_j/good'])) + ((1-bins['Cabin']['P_j/bad'])/(bins['Cabin']['bad'].sum()*bins['Cabin']['P_j/bad']))
bins['Cabin']['RR_IC_low'] = np.exp(bins['Cabin']['f*'] - stats.norm.ppf(.975, loc=0, scale=1)*np.sqrt(bins['Cabin']['V(f*)']))
bins['Cabin']['RR_IC_Hight'] = np.exp(bins['Cabin']['f*'] + stats.norm.ppf(.975, loc=0, scale=1)*np.sqrt(bins['Cabin']['V(f*)']))
bins['Cabin']
```

```{python}
sc.woebin_plot(bins)
```

**Dicas para discretização via WOE:**
1) Todas as categorias calculadas têm pelo menos 5% das observações.
2) Todas as categorias calculadas têm valores diferentes de zero para o número de casos bons e ruins.
3) Os Woe's são distintos para cada categoria calculada. As categorias calculadas que tinham Woe's semelhantes foram agrupadas.
4) Valores ausentes estão em categorias separadas.
5) Os Woe's são monotônicos, isto é, estão crescendo ou diminuindo com os agrupamentos. (Recomendação do uso dessa dica é mais fraca do que para as outras dicas).

```{python}
bins['Cabin'].bin
```

### Aplicando as modificações nos dados

```{python}
df_for_woe["Cabin"] = df_for_woe["Cabin"].replace({"A": "A%,%B%,%C", "B": "A%,%B%,%C", "C": "A%,%B%,%C",
                        "D":"D%,%E%,%F%,%G%,%T","E":"D%,%E%,%F%,%G%,%T","F":"D%,%E%,%F%,%G%,%T","G":"D%,%E%,%F%,%G%,%T","T":"D%,%E%,%F%,%G%,%T"})
```

```{python}
df_for_woe
```

```{python}
# Função estava com problemas no momento em que foi utilizado (não substitui os valores corretamente).
#df_for_woe_binnarized = sc.woebin_ply(df_for_woe, bins)
#df_for_woe_binnarized
```

## Embarked

```{python}
df_for_woe['Embarked'] = df_for_woe['Embarked'].cat.add_categories(['Missing'])
df_for_woe[['Embarked']] = df_for_woe[['Embarked']].fillna('Missing')
```

```{python}
calcula_tab_frequencia(tabela_de_entrada=df_for_woe[df_for_woe.Sample == 'train'],
                       nome_variavel='Embarked')
```

```{python}
df_for_woe[df_for_woe.Embarked == 'Missing']
```

Intuitivamente, poderia ser utilizado o método de imputação de dados ausentes pela classe mais frequente pois são poucos casos (2 observações). Além disso, em pesquisas pela internet, foi constatado que as duas mulheres embarcaram em Southampton (S). Portanto, faz sentido seguir com esse preenchimento dos dados.

```{python}
df_for_woe.loc[df_for_woe.PassengerId.isin([62,830]),'Embarked'] = df_for_woe.loc[df_for_woe.PassengerId.isin([62,830]),'Embarked'].replace({'Missing':'S'})
```

```{python}
df_for_woe[df_for_woe.PassengerId.isin([62,830])]
```

```{python}
calcula_tab_frequencia(tabela_de_entrada=df_for_woe[df_for_woe.Sample == 'train'],
                       nome_variavel='Embarked')
```

```{python}
grafico_tab_frequencia(table_count_prop=table_count_propEmbarked,
                       nome_variavel='Embarked',
                       cor_grafico='blue',
                       nudge_x_e=-0.225,
                       nudge_x_d=0.15,
                       size=10)
```

```{python}
calcula_tab_frequencia_bivariada(df_for_woe[df_for_woe.Sample == 'train'],'Survived','Embarked')
```

```{python}
grafico_tab_frequencia_bivariada2(table_count_propSurvived_Embarked,'Survived','Embarked',angle=0,size1=12,size2=12)
```

```{python}
grafico_tab_frequencia_bivariada3(table_count_propSurvived_Embarked,'Survived','Embarked',size1=12,size2=0,angle=0)
```

# Cria tabelas finais (com dados tratados)

```{python}
df_tidy = df_for_woe.copy().drop(columns=['Survived_BadGod'])
train_tidy = df_for_woe[df_for_woe.Sample == 'train'].drop(columns=['Survived_BadGod'])
test_tidy = df_for_woe[df_for_woe.Sample == 'test'].drop(columns=['Survived_BadGod'])
```

```{python}
#df_tidy.to_pickle("df_tidy.pkl")
df_tidy = pd.read_pickle("df_tidy.pkl")
#train_tidy.to_pickle("train_tidy.pkl")
train_tidy = pd.read_pickle("train_tidy.pkl")
#test_tidy.to_pickle("test_tidy.pkl")
test_tidy = pd.read_pickle("test_tidy.pkl")
```

```{python}
df_tidy.head()
```

# Tabela de Metadados (Foco em Modelagem Estatística/Machine Learning)

```{python}
tabela_metadados(tabela_de_entrada=train_tidy,
                 sufixo_da_tabela_saída='train_tidy',
                 lista_vars_drop=['PassengerId','Name','Ticket','Sample'])
```

```{python}
tabela_metadados(tabela_de_entrada=test_tidy,
                 sufixo_da_tabela_saída='test_tidy',
                 lista_vars_drop=['PassengerId','Name','Ticket','Sample','Survived'])
```

```{python}
tabela_metadados(tabela_de_entrada=df_tidy,
                 sufixo_da_tabela_saída='df_tidy',
                 lista_vars_drop=['PassengerId','Name','Ticket'])
```

```{python}
train_ml = train_tidy.copy()
test_ml = test_tidy.copy()

train_ml = train_ml[metadata_table_train_tidy[~metadata_table_train_tidy['drop']]['columns'].values]
test_ml = test_ml[metadata_table_test_tidy[~metadata_table_test_tidy['drop']]['columns'].values]
```

```{python}
train_ml
```

```{python}
test_ml
```

# Tratamento das variáveis para Modelagem Estatística/Machine Learning

## LabelEncoder (Utilizado para a variável resposta)

```{python}
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
train_ml['Survived'] = label_encoder.fit_transform(train_ml['Survived'])
```

## OrdinalEncoder (Utilizado para variáveis qualitativas ordinais)

```{python}
from sklearn.preprocessing import OrdinalEncoder
encoder = OrdinalEncoder()
# transform data
train_ml[['Pclass','Age']] = encoder.fit_transform(train_ml[['Pclass','Age']].astype('str'))
test_ml[['Pclass','Age']] = encoder.fit_transform(test_ml[['Pclass','Age']].astype('str'))
```

## Dummy Variable Encoding - **OneHotEncoder without the first category** (Utilizado para variáveis qualitativas nominais)

```{python}
from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder(drop='first', sparse=False)

columns_dummy = ['Sex','Cabin','Embarked']

train_ml_encoded = encoder.fit_transform(train_ml[columns_dummy])
column_names = encoder.get_feature_names(columns_dummy)
train_ml_encoded = pd.DataFrame(train_ml_encoded, columns= column_names)

test_ml_encoded = encoder.fit_transform(test_ml[columns_dummy])
column_names = encoder.get_feature_names(columns_dummy)
test_ml_encoded = pd.DataFrame(test_ml_encoded, columns= column_names)

train_ml = train_ml.drop(columns=columns_dummy)
test_ml = test_ml.drop(columns=columns_dummy)
```

```{python}
train_ml = pd.concat([train_ml,train_ml_encoded],axis=1)
train_ml
```

```{python}
test_ml = pd.concat([test_ml,test_ml_encoded],axis=1)
test_ml
```

# Feature Selection

## RFECV - Logistic Regression

```{python}
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import KFold
from sklearn.feature_selection import RFECV

# Create the RFE object and compute a cross-validated score.
lr = LogisticRegression(max_iter=500)
# The "accuracy" scoring is proportional to the number of correct
# classifications

# Definindo os valores para os folds
num_folds = 100
seed = 7

# Separando os dados em folds
kfold = KFold(num_folds, True, random_state = seed)

min_features_to_select = 1  # Minimum number of features to consider
rfecv = RFECV(estimator=lr, step=1, cv=kfold,
              scoring='accuracy',
              min_features_to_select=min_features_to_select)
rfecv.fit(train_ml.drop(columns='Survived'), train_ml['Survived'])

print("Optimal number of features : %d" % rfecv.n_features_)

# Plot number of features VS. cross-validation scores
plt.figure()
plt.xlabel("Number of features selected")
plt.ylabel("Cross validation score (nb of correct classifications)")
plt.plot(range(min_features_to_select,
               len(rfecv.grid_scores_) + min_features_to_select),
         rfecv.grid_scores_)
plt.show()
```

```{python}
print(rfecv.support_)
print(rfecv.ranking_)
f = rfecv.get_support(1) #the most important features
names = train_ml.drop(columns='Survived').columns[f] # final features`
names
```

```{python}
rfecv.grid_scores_.max()
```

Apesar do método de seleção de variável ter selecionado apenas algumas categorias da variável Embarked, (Para Embarked, sobrou somente Embarked_S e não Embarked_Q), todas as categorias foram adicionadas no modelo. A única variável que não foi selecionada é Fare.

```{python}
names_best_variables = list()
for i in range(0,len(names)):   
    names_best_variables.insert(i, names.str.split('_')[i][0]) # Adicionar o nome das variaveis antes de '_'
names_best_variables = list(dict.fromkeys(names_best_variables)) # remover duplicatas
names_best_variables
```

```{python}
train_ml = train_ml.drop(columns='Fare')
train_ml
```

```{python}
test_ml = test_ml.drop(columns='Fare')
test_ml
```

# Avaliando a Performance

## Algoritmos de Classificação

### Métricas para Algoritmos de Classificação

https://scikit-learn.org/stable/modules/model_evaluation.html

# Seleção do Modelo Preditivo

```{python}
# Import dos módulos
from pandas import read_csv
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier

max_iter=500
num_trees = 100

# Definindo os valores para o número de folds
num_folds = 100
seed = 7

# Preparando a lista de modelos
modelos = []
modelos.append(('LR', LogisticRegression(max_iter=max_iter)))
modelos.append(('LDA', LinearDiscriminantAnalysis()))
modelos.append(('NB', GaussianNB()))
modelos.append(('KNN', KNeighborsClassifier()))
modelos.append(('SVM', SVC()))
modelos.append(('CART', DecisionTreeClassifier()))
modelos.append(('Bagging',BaggingClassifier(base_estimator = DecisionTreeClassifier(),
                                                      n_estimators = num_trees, random_state = seed)))
modelos.append(('RandomForest',RandomForestClassifier(n_estimators = num_trees, max_features = 'auto', random_state = seed)))
modelos.append(('AdaBoost',AdaBoostClassifier(base_estimator = DecisionTreeClassifier(),
                                                      n_estimators = num_trees, random_state = seed)))


# Avaliando cada modelo em um loop
resultados = []
nomes = []

for nome, modelo in modelos:
    kfold = KFold(n_splits = num_folds, shuffle=True, random_state = seed)
    cv_results = cross_val_score(modelo, train_ml.drop(columns='Survived'), train_ml['Survived'], cv = kfold, scoring = 'accuracy')
    resultados.append(cv_results)
    nomes.append(nome)
    msg = "%s: %f (%f)" % (nome, cv_results.mean(), cv_results.std())
    print(msg)

# Boxplot para comparar os algoritmos
fig = plt.figure()
fig.suptitle('Comparação de Algoritmos de Classificação')
ax = fig.add_subplot(111)
plt.boxplot(resultados)
ax.set_xticklabels(nomes)
plt.show()
```

## Otimização do Modelo - Ajuste de Hyperparâmetros

### Grid Search Parameter Tuning

```{python}
# Import dos módulos
from pandas import read_csv
from sklearn.model_selection import GridSearchCV

# Definindo os valores que serão testados
valores_grid = {'C': [0.1, 1, 10, 100, 1000], 
              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],
              'kernel': ['rbf']} 

# Criando o modelo
modelo = SVC()

# Criando o grid
grid = GridSearchCV(estimator = modelo, param_grid = valores_grid)
grid.fit(train_ml.drop(columns='Survived'), train_ml['Survived'])

# Print do resultado
print("Acurácia: %.3f" % (grid.best_score_ * 100))
print("Melhores Parâmetros do Modelo:\n", grid.best_estimator_)
```

### Métricas para Algoritmos de Classificação

```{python}
# Salvando o resultado do seu trabalho
from pandas import read_csv
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report


# Definindo o tamanho do conjunto de dados
teste_size = 0.33
seed = 7

# Criando o dataset de treino e de teste
X_treino, X_teste, Y_treino, Y_teste = train_test_split(train_ml.drop(columns='Survived'),
                                       train_ml['Survived'], test_size = teste_size, random_state = seed)
```

```{python}
# Criando o modelo
modelo = grid.best_estimator_

# Treinando o modelo
modelo.fit(X_treino, Y_treino)

# Fazendo as previsões e construindo o relatório
previsoes = modelo.predict(X_treino)
report = classification_report(Y_treino, previsoes)

# Imprimindo o relatório
print(report)
```

```{python}
# Criando o modelo
modelo = grid.best_estimator_

# Treinando o modelo
modelo.fit(X_teste, Y_teste)

# Fazendo as previsões e construindo o relatório
previsoes = modelo.predict(X_teste)
report = classification_report(Y_teste, previsoes)

# Imprimindo o relatório
print(report)
```

# Salvando o resultado do seu trabalho

```{python}
import pickle

# Salvando o modelo
arquivo = 'modelo_classificador_final.sav'
pickle.dump(modelo, open(arquivo, 'wb'))
print("Modelo salvo!")
```

```{python}
# Carregando o arquivo
modelo_classificador_final = pickle.load(open(arquivo, 'rb'))
print("Modelo carregado!")
```

```{python}
modelo_classificador_final.predict(test_ml)
```

```{python}
test = pd.read_csv('../test.csv',sep = ",")
test
```

```{python}
predict_submission = pd.concat([test['PassengerId'],
                pd.DataFrame({'Survived':modelo_classificador_final.predict(test_ml)})],axis=1)
predict_submission.to_csv('predict_submission.csv',index=False,sep=',')
predict_submission
```

